{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV DNN - Part 1\n",
    "\n",
    "- OpenCV DNN\n",
    "- Yolo Darknet\n",
    "- OpenCV DNN Yolo\n",
    "- Train Yolo Model with Custom Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Original Source : https://github.com/alitourani/yolo-license-plate-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro Deep Learning\n",
    "\n",
    "- “#1 Pengenalan Machine Learning”  https://medium.com/@yunusmuhammad007/pengenalan-machine-learning-2320b5ca7266\n",
    "- “#2 Supervised VS Unsupervised VS Reinforcement ML”  https://link.medium.com/wEcGRj3gq5\n",
    "- “#3 Machine Learning Evaluation”  https://link.medium.com/qJ9Kd26gq5\n",
    "- “#4 Alat dan Bahan untuk Machine Learning”  https://medium.com/@yunusmuhammad007/3-alat-dan-bahan-untuk-machine-learning-92c717286624\n",
    "- “#5 Basic Python Programming”  https://medium.com/@yunusmuhammad007/5-basic-python-programming-87c89e1d0d3e\n",
    "- “#6 Artificial Neural Network (ANN) — Part 1 (Pengenalan)”  https://link.medium.com/TbaRUcJZv5\n",
    "- “#7 Artificial Neural Network (ANN) — Part 2 (Single Layer Perceptron)”  https://link.medium.com/kpBiXHBdz5\n",
    "- “#8 Artificial Neural Network (ANN) — Part 3 (Teori Dasar Multi Layer Perceptron Backpropagation)”  https://link.medium.com/D7rAjn69F6\n",
    "- “#9 Artificial Neural Network (ANN) — Part 4 (MLP Backpropagation Time Series Forecasting…”  https://link.medium.com/s2ZZFy89F6\n",
    "- “#10 Artificial Neural Network (ANN) — Part 5 (Time Series Forecasting ISPU CO DKI Jakarta…”  https://link.medium.com/ccHKkBaaG6\n",
    "- “#11 Artificial Neural Network (ANN) — Part 6 Konsep Dasar Convolutional Neural Network (CNN)”  https://link.medium.com/gy2J4beaG6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenCV DNN\n",
    "\n",
    "- Compatibility : > OpenCV 3.3\n",
    "- Wiki : https://github.com/opencv/opencv/wiki/Deep-Learning-in-OpenCV\n",
    "- Since OpenCV 3.1 there is DNN module in the library that implements **forward pass** (inferencing) with deep networks, **pre-trained**using some popular deep learning frameworks.\n",
    "- The supported frameworks:\n",
    "    - Caffe\n",
    "    - TensorFlow\n",
    "    - Torch\n",
    "    - Darknet (Yolo)\n",
    "    - Models in ONNX format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yolo (neural networks for object detection)\n",
    "\n",
    "https://github.com/AlexeyAB/darknet#pre-trained-models\n",
    "\n",
    "- Paper Yolo v4: https://arxiv.org/abs/2004.10934\n",
    "- More details: [medium link](https://medium.com/@alexeyab84/yolov4-the-most-accurate-real-time-neural-network-on-ms-coco-dataset-73adfd3602fe?source=friends_link&sk=6039748846bbcf1d960c3061542591d7)\n",
    "- Manual: https://github.com/AlexeyAB/darknet/wiki\n",
    "- Result : https://www.youtube.com/user/pjreddie/videos <br>\n",
    "<img src=\"resource/opencv_dnn.gif\" style=\"width:500px\"></img>\n",
    "<br>\n",
    "<img src=\"resource/82835867-f1c62380-9ecd-11ea-9134-1598ed2abc4b.png\" style=\"width:700px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCV Yolo DNN\n",
    "\n",
    "- Required files :\n",
    "    - `.names` file\n",
    "    - `.cfg` file\n",
    "    - `.weights` file  \n",
    "- **cfg-files** - are structures of neural networks: https://github.com/AlexeyAB/darknet/tree/master/cfg\n",
    "- **weights-files** - are weights for correspond cfg-file, can be downloaded from: https://pjreddie.com/darknet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read class names from `.names` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['License Plate']\n"
     ]
    }
   ],
   "source": [
    "classesFile = \"yolo/plate_number/plate_number.names\"\n",
    "\n",
    "classes = None\n",
    "with open(classesFile, 'rt') as f:\n",
    "    classes = f.read().rstrip('\\n').split('\\n')\n",
    "    \n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load `.cfg` and `.weights` using `cv2.dnn.readNetFromDarknet()`\n",
    "- Download `.weights` file from [this](https://drive.google.com/file/d/1vXjIoRWY0aIpYfhj3TnPUGdmJoHnWaOc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelConfiguration = \"yolo/plate_number/plate_number_yolov3.cfg\"\n",
    "modelWeights = \"yolo/plate_number/plate_number_yolov3.weights\"\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(modelConfiguration, modelWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cv2.dnn_Net"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenCV DNN Backend\n",
    "- Set OpenCV DNN Backend using `.setPreferableBackend(backend_type)`\n",
    "- where `backend_type` :\n",
    "    - `cv2.dnn.DNN_BACKEND_DEFAULT`\n",
    "    - `cv2.dnn.DNN_BACKEND_VKOM`\n",
    "    - `cv2.dnn.DNN_BACKEND_HALIDE`\n",
    "    - `cv2.dnn.DNN_BACKEND_OPENCV`\n",
    "    - `cv2.dnn.DNN_BACKEND_INFERENCE_ENGINE`\n",
    "      \n",
    "#### OpenCV DNN Target\n",
    "- Set OpenCV DNN Target using `.setPreferableTarget(target_type)`\n",
    "- where `target_type` :\n",
    "    - `cv2.dnn.DNN_TARGET_CPU`\n",
    "    - `cv2.dnn.DNN_TARGET_MYRIAD`\n",
    "    - `cv2.dnn.DNN_TARGET_OPENCL`\n",
    "    - `cv2.dnn.DNN_TARGET_FPGA`\n",
    "    - `cv2.dnn.DNN_TARGET_VULKAN`\n",
    "    - `cv2.dnn.DNN_TARGET_CUDA` > OpenCV 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert image to blob using `cv2.dnn.blobFromImage()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpWidth = 416      # Width of network's input image\n",
    "inpHeight = 416     # Height of network's input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = cv2.imread(\"dataset/mobil3.jpg\")\n",
    "blob = cv2.dnn.blobFromImage(\n",
    "                        frame, \n",
    "                        1/255, \n",
    "                        (inpWidth, inpHeight), \n",
    "                        [0, 0, 0],\n",
    "                        1, \n",
    "                        crop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (1, 3, 416, 416), dtype('float32'))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(blob), blob.shape, blob.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((194, 259, 3), dtype('uint8'))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.shape, frame.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get Output layer using `.getUnconnectedOutLayersNames()` from `net` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conv_0',\n",
       " 'bn_0',\n",
       " 'relu_1',\n",
       " 'conv_1',\n",
       " 'bn_1',\n",
       " 'relu_2',\n",
       " 'conv_2',\n",
       " 'bn_2',\n",
       " 'relu_3',\n",
       " 'conv_3',\n",
       " 'bn_3',\n",
       " 'relu_4',\n",
       " 'shortcut_4',\n",
       " 'conv_5',\n",
       " 'bn_5',\n",
       " 'relu_6',\n",
       " 'conv_6',\n",
       " 'bn_6',\n",
       " 'relu_7',\n",
       " 'conv_7',\n",
       " 'bn_7',\n",
       " 'relu_8',\n",
       " 'shortcut_8',\n",
       " 'conv_9',\n",
       " 'bn_9',\n",
       " 'relu_10',\n",
       " 'conv_10',\n",
       " 'bn_10',\n",
       " 'relu_11',\n",
       " 'shortcut_11',\n",
       " 'conv_12',\n",
       " 'bn_12',\n",
       " 'relu_13',\n",
       " 'conv_13',\n",
       " 'bn_13',\n",
       " 'relu_14',\n",
       " 'conv_14',\n",
       " 'bn_14',\n",
       " 'relu_15',\n",
       " 'shortcut_15',\n",
       " 'conv_16',\n",
       " 'bn_16',\n",
       " 'relu_17',\n",
       " 'conv_17',\n",
       " 'bn_17',\n",
       " 'relu_18',\n",
       " 'shortcut_18',\n",
       " 'conv_19',\n",
       " 'bn_19',\n",
       " 'relu_20',\n",
       " 'conv_20',\n",
       " 'bn_20',\n",
       " 'relu_21',\n",
       " 'shortcut_21',\n",
       " 'conv_22',\n",
       " 'bn_22',\n",
       " 'relu_23',\n",
       " 'conv_23',\n",
       " 'bn_23',\n",
       " 'relu_24',\n",
       " 'shortcut_24',\n",
       " 'conv_25',\n",
       " 'bn_25',\n",
       " 'relu_26',\n",
       " 'conv_26',\n",
       " 'bn_26',\n",
       " 'relu_27',\n",
       " 'shortcut_27',\n",
       " 'conv_28',\n",
       " 'bn_28',\n",
       " 'relu_29',\n",
       " 'conv_29',\n",
       " 'bn_29',\n",
       " 'relu_30',\n",
       " 'shortcut_30',\n",
       " 'conv_31',\n",
       " 'bn_31',\n",
       " 'relu_32',\n",
       " 'conv_32',\n",
       " 'bn_32',\n",
       " 'relu_33',\n",
       " 'shortcut_33',\n",
       " 'conv_34',\n",
       " 'bn_34',\n",
       " 'relu_35',\n",
       " 'conv_35',\n",
       " 'bn_35',\n",
       " 'relu_36',\n",
       " 'shortcut_36',\n",
       " 'conv_37',\n",
       " 'bn_37',\n",
       " 'relu_38',\n",
       " 'conv_38',\n",
       " 'bn_38',\n",
       " 'relu_39',\n",
       " 'conv_39',\n",
       " 'bn_39',\n",
       " 'relu_40',\n",
       " 'shortcut_40',\n",
       " 'conv_41',\n",
       " 'bn_41',\n",
       " 'relu_42',\n",
       " 'conv_42',\n",
       " 'bn_42',\n",
       " 'relu_43',\n",
       " 'shortcut_43',\n",
       " 'conv_44',\n",
       " 'bn_44',\n",
       " 'relu_45',\n",
       " 'conv_45',\n",
       " 'bn_45',\n",
       " 'relu_46',\n",
       " 'shortcut_46',\n",
       " 'conv_47',\n",
       " 'bn_47',\n",
       " 'relu_48',\n",
       " 'conv_48',\n",
       " 'bn_48',\n",
       " 'relu_49',\n",
       " 'shortcut_49',\n",
       " 'conv_50',\n",
       " 'bn_50',\n",
       " 'relu_51',\n",
       " 'conv_51',\n",
       " 'bn_51',\n",
       " 'relu_52',\n",
       " 'shortcut_52',\n",
       " 'conv_53',\n",
       " 'bn_53',\n",
       " 'relu_54',\n",
       " 'conv_54',\n",
       " 'bn_54',\n",
       " 'relu_55',\n",
       " 'shortcut_55',\n",
       " 'conv_56',\n",
       " 'bn_56',\n",
       " 'relu_57',\n",
       " 'conv_57',\n",
       " 'bn_57',\n",
       " 'relu_58',\n",
       " 'shortcut_58',\n",
       " 'conv_59',\n",
       " 'bn_59',\n",
       " 'relu_60',\n",
       " 'conv_60',\n",
       " 'bn_60',\n",
       " 'relu_61',\n",
       " 'shortcut_61',\n",
       " 'conv_62',\n",
       " 'bn_62',\n",
       " 'relu_63',\n",
       " 'conv_63',\n",
       " 'bn_63',\n",
       " 'relu_64',\n",
       " 'conv_64',\n",
       " 'bn_64',\n",
       " 'relu_65',\n",
       " 'shortcut_65',\n",
       " 'conv_66',\n",
       " 'bn_66',\n",
       " 'relu_67',\n",
       " 'conv_67',\n",
       " 'bn_67',\n",
       " 'relu_68',\n",
       " 'shortcut_68',\n",
       " 'conv_69',\n",
       " 'bn_69',\n",
       " 'relu_70',\n",
       " 'conv_70',\n",
       " 'bn_70',\n",
       " 'relu_71',\n",
       " 'shortcut_71',\n",
       " 'conv_72',\n",
       " 'bn_72',\n",
       " 'relu_73',\n",
       " 'conv_73',\n",
       " 'bn_73',\n",
       " 'relu_74',\n",
       " 'shortcut_74',\n",
       " 'conv_75',\n",
       " 'bn_75',\n",
       " 'relu_76',\n",
       " 'conv_76',\n",
       " 'bn_76',\n",
       " 'relu_77',\n",
       " 'conv_77',\n",
       " 'bn_77',\n",
       " 'relu_78',\n",
       " 'conv_78',\n",
       " 'bn_78',\n",
       " 'relu_79',\n",
       " 'conv_79',\n",
       " 'bn_79',\n",
       " 'relu_80',\n",
       " 'conv_80',\n",
       " 'bn_80',\n",
       " 'relu_81',\n",
       " 'conv_81',\n",
       " 'permute_82',\n",
       " 'yolo_82',\n",
       " 'identity_83',\n",
       " 'conv_84',\n",
       " 'bn_84',\n",
       " 'relu_85',\n",
       " 'upsample_85',\n",
       " 'concat_86',\n",
       " 'conv_87',\n",
       " 'bn_87',\n",
       " 'relu_88',\n",
       " 'conv_88',\n",
       " 'bn_88',\n",
       " 'relu_89',\n",
       " 'conv_89',\n",
       " 'bn_89',\n",
       " 'relu_90',\n",
       " 'conv_90',\n",
       " 'bn_90',\n",
       " 'relu_91',\n",
       " 'conv_91',\n",
       " 'bn_91',\n",
       " 'relu_92',\n",
       " 'conv_92',\n",
       " 'bn_92',\n",
       " 'relu_93',\n",
       " 'conv_93',\n",
       " 'permute_94',\n",
       " 'yolo_94',\n",
       " 'identity_95',\n",
       " 'conv_96',\n",
       " 'bn_96',\n",
       " 'relu_97',\n",
       " 'upsample_97',\n",
       " 'concat_98',\n",
       " 'conv_99',\n",
       " 'bn_99',\n",
       " 'relu_100',\n",
       " 'conv_100',\n",
       " 'bn_100',\n",
       " 'relu_101',\n",
       " 'conv_101',\n",
       " 'bn_101',\n",
       " 'relu_102',\n",
       " 'conv_102',\n",
       " 'bn_102',\n",
       " 'relu_103',\n",
       " 'conv_103',\n",
       " 'bn_103',\n",
       " 'relu_104',\n",
       " 'conv_104',\n",
       " 'bn_104',\n",
       " 'relu_105',\n",
       " 'conv_105',\n",
       " 'permute_106',\n",
       " 'yolo_106']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.getLayerNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerOutput = net.getUnconnectedOutLayersNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yolo_82', 'yolo_94', 'yolo_106']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layerOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set blob to input network using `.setInput()` on `net` object\n",
    "- Do forward pass and get output using `.forward()` on `net` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.setInput(blob)\n",
    "outs = net.forward(layerOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs[0][0][5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "classId = np.argmax(outs[0][0][5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for out in outs:\n",
    "frame_h, frame_w, frame_c = frame.shape\n",
    "classIds = []\n",
    "confidences = []\n",
    "boxes = []\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        classId = np.argmax(scores)\n",
    "        confidence = scores[classId]\n",
    "        c_x = int(detection[0] * frame_w)\n",
    "        c_y = int(detection[1] * frame_h)\n",
    "        w = int(detection[2] * frame_w)\n",
    "        h = int(detection[3] * frame_h)\n",
    "        x = int(c_x - w / 2)\n",
    "        y = int(c_y - h / 2)\n",
    "        classIds.append(classId)\n",
    "        confidences.append(float(confidence))\n",
    "        boxes.append([x, y, w, h])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find Non maximum supression (NMS) from boxes using `cv2.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "confThreshold = 0.5  # confidence level threshold\n",
    "nmsThreshold = 0.4 # NMS threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = cv2.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold)\n",
    "for i in indices:\n",
    "    i = i[0]\n",
    "    box = boxes[i]\n",
    "    x = box[0]\n",
    "    y = box[1]\n",
    "    w = box[2]\n",
    "    h = box[3]\n",
    "\n",
    "    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        \n",
    "cv2.imshow(\"frame\", frame)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap all into function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawPred(classId, conf, left, top, right, bottom):\n",
    "    cv2.rectangle(frame, (left, top), (right, bottom), (255, 0, 255), 2)\n",
    "\n",
    "    label = '%.2f' % conf\n",
    "    if classes:\n",
    "        assert(classId < len(classes))\n",
    "        label = '%s: %s' % (classes[classId], label)\n",
    "\n",
    "    labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "    top = max(top, labelSize[1])\n",
    "    cv2.rectangle(frame, (left, top - round(1.5*labelSize[1])), (max(right, left + labelSize[0]), top + baseLine), (255, 0, 255), -1)\n",
    "    cv2.putText(frame, label, (left, top), cv2.FONT_HERSHEY_SIMPLEX, 0.50, (255, 255, 255), 1)\n",
    "\n",
    "def postprocess(frame, outs):\n",
    "    frame_h, frame_w, frame_c = frame.shape\n",
    "    classIds = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            classId = np.argmax(scores)\n",
    "            confidence = scores[classId]\n",
    "            if confidence > confThreshold:        \n",
    "                scores = detection[5:]\n",
    "                classId = np.argmax(scores)\n",
    "                confidence = scores[classId]\n",
    "                c_x = int(detection[0] * frame_w)\n",
    "                c_y = int(detection[1] * frame_h)\n",
    "                w = int(detection[2] * frame_w)\n",
    "                h = int(detection[3] * frame_h)\n",
    "                x = int(c_x - w / 2)\n",
    "                y = int(c_y - h / 2)\n",
    "                classIds.append(classId)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([x, y, w, h])\n",
    "\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold)\n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        box = boxes[i]\n",
    "        left = box[0]\n",
    "        top = box[1]\n",
    "        width = box[2]\n",
    "        height = box[3]\n",
    "        drawPred(classIds[i], confidences[i], left,\n",
    "                 top, left + width, top + height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_inf_time(frame, label, left =25, top=50):\n",
    "    labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "    cv2.rectangle(frame, (left, top - round(1.5*labelSize[1])), (left + round(1.5*labelSize[0]), top + baseLine), (0, 255, 255), -1)\n",
    "    cv2.putText(frame, label, (left, top), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"dataset/\" \n",
    "layerOutput = net.getUnconnectedOutLayersNames()\n",
    "for filename in os.listdir(dataset):\n",
    "    if os.path.splitext(filename)[1] != \".jpg\" :\n",
    "        continue\n",
    "    frame = cv2.imread(os.path.join(dataset, filename))\n",
    "    blob = cv2.dnn.blobFromImage(\n",
    "        frame, 1/255, (inpWidth, inpHeight), [0, 0, 0], 1, crop=False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(layerOutput)\n",
    "\n",
    "    postprocess(frame, outs)\n",
    "\n",
    "    t, _ = net.getPerfProfile()\n",
    "    label = 'Inference time: %.2f s' % (t / cv2.getTickFrequency())\n",
    "\n",
    "    frame = draw_inf_time(frame, label)\n",
    "    \n",
    "    cv2.imshow(\"out\", frame)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"cars_on_the_road_2.mp4\")\n",
    "while cap.isOpened :\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        continue\n",
    "    blob = cv2.dnn.blobFromImage(\n",
    "        frame, 1/255, (inpWidth, inpHeight), [0, 0, 0], 1, crop=False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(getOutputsNames(net))\n",
    "\n",
    "    postprocess(frame, outs)\n",
    "\n",
    "    t, _ = net.getPerfProfile()\n",
    "    label = 'Inference time: %.2f s' % (t / cv2.getTickFrequency())\n",
    "    cv2.putText(frame, label, (0, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255))\n",
    "\n",
    "    cv2.imshow(\"out\", frame)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Yolo Using Your Custom Dataset\n",
    "\n",
    "- **Prepare Dataset** : 100 image car (just for testing, in real case, need a lot of data)\n",
    "- **Dataset Annotation** : To annotante dataset using Yolo Format, use this desktop app : **[LabelImg]( https://github.com/tzutalin/labelImg)**\n",
    "<img src=\"resource/demo3.jpg\" style=\"width:600px\"></img>\n",
    "- **Train Model** on Colab ( https://colab.research.google.com/drive/1lTGZsfMaGUpBG4inDIQwIJVW476ibXk_#scrollTo=WewV3jU3B4Eo )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:opencv_env]",
   "language": "python",
   "name": "conda-env-opencv_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
