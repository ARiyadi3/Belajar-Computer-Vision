{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV - Part 7\n",
    "\n",
    "- Image Pyramid \n",
    "- Geometric Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Pyramid\n",
    "\n",
    "- An image pyramid is a collection of images - all arising from a single original image - that are successively downsampled until some desired stopping point is reached.\n",
    "- There are two common kinds of image pyramids:\n",
    "    - **Gaussian pyramid**: Used to **downsample** images\n",
    "    - **Laplacian pyramid**: Used to reconstruct an **upsampled** image from an image lower in the pyramid (with less resolution)\n",
    "\n",
    "#### Gaussian Pyramid\n",
    "- Imagine the pyramid as a set of layers in which the higher the layer, the smaller the size.\n",
    "- Every layer is numbered from bottom to top, so layer ($i+1$) (denoted as $G_i+1$ is smaller than layer $i ( G_i)$.<br>\n",
    "<img src=\"resource/Pyramids_Tutorial_Pyramid_Theory.png\" style=\"width:300px\"></img>\n",
    "- To produce layer ($i+1$) in the Gaussian pyramid, we do the following:\n",
    "    - **Convolve** $G_i$ with a *Gaussian kernel*:<br>\n",
    "    $\\frac{1}{16} \\begin{bmatrix} 1 & 4 & 6 & 4 & 1 \\\\ 4 & 16 & 24 & 16 & 4 \\\\ 6 & 24 & 36 & 24 & 6 \\\\ 4 & 16 & 24 & 16 & 4 \\\\ 1 & 4 & 6 & 4 & 1 \\end{bmatrix}$\n",
    "    - Remove every even-numbered row and column.   \n",
    "- The resulting image will be exactly *one-quarter* the area of its predecessor. Iterating this process on the input image $G_0$ (original image) produces the entire pyramid.\n",
    "- The procedure above was useful to **downsample** an image. What if we want to make it bigger?\n",
    "    - First, **upsize** the image to twice the original in each dimension, with the new even rows and\n",
    "    - Perform a**convolution** with the same kernel shown above (multiplied by 4) to approximate the values of the \"missing pixels\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Downsampling and upsampling method in OpenCV is used `cv2.pyrUp(img, dstSize, borderType)` and `cv2.pyrDown(img, dstSize, borderType)`\n",
    "- where :\n",
    "    - `img` : input image\n",
    "    - `dstSize` : image destination size, default ($0.5 x w, 0.5 x h$) for downscale, ($2 x w, 2 x h$) for upscale.\n",
    "    - `borderType` :\n",
    "        - `cv2.BORDER_DEFAULT`\n",
    "        - `cv2.BORDER_CONSTANT`\n",
    "        - `cv2.BORDER_REPLICATE`\n",
    "        - `cv2.BORDER_REFLECT`\n",
    "        - `cv2.BORDER_WRAP`\n",
    "        - `cv2.BORDER_ISOLATED`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- downscal & upscaling `lena.jpg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"lena.jpg\")\n",
    "img_PD = cv2.pyrDown(img)\n",
    "\n",
    "cv2.imshow(\"pyramid downscale 1/4\", img_PD)\n",
    "cv2.imshow(\"original\", img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_PU = cv2.pyrUp(img_PD)\n",
    "\n",
    "cv2.imshow(\"pyramid upscale 4x\", img_PU)\n",
    "cv2.imshow(\"original\", img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- multiple downscale `lena.jpg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"lena.jpg\")\n",
    "h, w, c = img.shape\n",
    "print(\"image - %d : %d,%d\" % (0, h, w))\n",
    "cv2.imshow(\"image - %d\" % 0, img)\n",
    "\n",
    "for i in range (1, 10):\n",
    "    img = cv2.pyrDown(img)\n",
    "    h, w, c = img.shape\n",
    "    if h < 100 and w < 100 :\n",
    "        break\n",
    "    print(\"image - %d : %d,%d\" % (i, h, w))\n",
    "    cv2.imshow(\"image - %d\" % i, img)\n",
    "    \n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  multiple upscale `lena.jpg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (1, 10):\n",
    "    img = cv2.pyrUp(img)\n",
    "    h, w, c = img.shape\n",
    "    if h > 512 and w > 512 :\n",
    "        break\n",
    "    print(\"image - %d : %d,%d\" % (i, h, w))\n",
    "    cv2.imshow(\"image - %d\" % i, img)\n",
    "    \n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Basic Image Operation Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image addition\n",
    "- method : `cv2.add(img1, img2)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = np.ones((5, 5, 3)).astype(np.uint8)*50 # black image\n",
    "img2 = np.ones((5, 5, 3)).astype(np.uint8)*127 # white image\n",
    "\n",
    "out = cv2.add(img1, img2)\n",
    "\n",
    "print(out[:,:,0])\n",
    "plt.imshow(out[:,:, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image substraction\n",
    "- method : `cv2.subtract(img1, img2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = np.ones((5, 5, 3)).astype(np.uint8)*200 # black image\n",
    "img2 = np.ones((5, 5, 3)).astype(np.uint8)*150 # white image\n",
    "\n",
    "out = cv2.subtract(img1, img2)\n",
    "\n",
    "print(out[:,:,0])\n",
    "plt.imshow(out[:,:, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging image (numpy)\n",
    "### horizontal merging\n",
    "- `np.hstack((img1, img2))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = np.ones((5, 5, 3)).astype(np.uint8)*200 # black image\n",
    "img2 = np.ones((5, 5, 3)).astype(np.uint8)*150 # white image\n",
    "\n",
    "out = np.hstack((img1, img2))\n",
    "\n",
    "print(out[:,:,0])\n",
    "plt.imshow(out[:,:, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vertical merging\n",
    "- `np.vstack((img1, img2))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = np.ones((5, 5, 3)).astype(np.uint8)*200 # black image\n",
    "img2 = np.ones((5, 5, 3)).astype(np.uint8)*150 # white image\n",
    "\n",
    "out = np.vstack((img1, img2))\n",
    "\n",
    "print(out[:,:,0])\n",
    "plt.imshow(out[:,:, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplacian Pyramid\n",
    "\n",
    "- Laplacian Pyramid dapat digunakan untuk edge detection.\n",
    "- Layer $i$ pada Laplacian Pyramid ($L_i$) dibangun dari layer pada Gaussian Pyramid ke $i$ ($G_i$) di kurangi hasil `cv2.pyrUp()` untuk layer Gaussian Pyramid ke $i + 1$ (($G_{i+1}$)).<br>\n",
    "$L_i = G_i - pyrUp(G_{i+1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GP_0 = cv2.imread(\"lena.jpg\") # 512x512\n",
    "GP_1 = cv2.pyrDown(GP_0) # 256x256\n",
    "LP_0 = cv2.subtract(GP_0, cv2.pyrUp(GP_1)) # 512x512\n",
    "\n",
    "cv2.imshow(\"GP 0\", GP_0)\n",
    "cv2.imshow(\"LP 0\", LP_0)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- multiple laplacian layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"lena.jpg\")\n",
    "\n",
    "GP = img.copy()\n",
    "print(GP.shape)\n",
    "GP_list = [GP] # Gaussina Pyramid `pyrDown` list image\n",
    "for i in range (0, 3):\n",
    "    GP = cv2.pyrDown(GP)\n",
    "    GP_list.append(GP)\n",
    "    print(GP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, GP in enumerate(GP_list):\n",
    "    cv2.imshow(\"GP image - %d\" % i, GP)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- kita memiliki, <br>\n",
    "    $GP_0$ : 512x512 (original image)<br>\n",
    "    $GP_1$ : 256x256 <br>\n",
    "    $GP_2$ : 128x128 <br>\n",
    "    $GP_3$ : 64x64 <br>\n",
    "- Selanjutnya, kita akan hitung $LP_i$,<br>\n",
    "    $LP_0$ : 64x64 (lowest GP image $GP_3$) <br>\n",
    "    $LP_1$ : 128x128 ($GP_2 - pyrUp(GP_3)$) <br>\n",
    "    $LP_2$ : 256x256 ($GP_1 - pyrUp(GP_2)$) <br>\n",
    "    $LP_3$ : 512x512 ($GP_0 - pyrUp(GP_1)$) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LP_list = [GP_list[-1]] # laplacian Pyramid list image\n",
    "print(LP_list[0].shape)\n",
    "for i in range (3, 0, -1):\n",
    "    LP =  cv2.subtract(GP_list[i-1], cv2.pyrUp(GP_list[i]))\n",
    "    LP_list.append(LP)\n",
    "    print(LP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, LP in enumerate(LP_list):\n",
    "    cv2.imshow(\"LP image - %d\" % i, LP)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### penerapan Image Pyramid untuk Image Stitching\n",
    "\n",
    "- Simple image stitching (but it may not look good due to discontinuities between images. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread(\"apple.jpg\")\n",
    "img2 = cv2.imread(\"orange.jpg\")\n",
    "\n",
    "h, w, c = img1.shape\n",
    "print(h,w,c)\n",
    "\n",
    "output = np.zeros((h,w,c)).astype(np.uint8)\n",
    "output[:, :w//2] = img1[:, :w//2]\n",
    "output[:, w//2:] = img2[:, w//2:]\n",
    "\n",
    "cv2.imshow(\"stiching image\", output)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stitching using Image Pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread(\"apple.jpg\")\n",
    "img2 = cv2.imread(\"orange.jpg\")\n",
    "\n",
    "GP1 = img1.copy()\n",
    "GP1_list = [GP1] \n",
    "for i in range (6):\n",
    "    GP1 = cv2.pyrDown(GP1)\n",
    "    GP1_list.append(GP1)\n",
    "\n",
    "GP2 = img2.copy()\n",
    "GP2_list = [GP2] \n",
    "for i in range (6):\n",
    "    GP2 = cv2.pyrDown(GP2)\n",
    "    GP2_list.append(GP2)\n",
    "    \n",
    "LP1_list = [GP1_list[-1]]\n",
    "for i in range (6, 0, -1):\n",
    "    LP1 =  cv2.subtract(GP1_list[i-1], cv2.pyrUp(GP1_list[i]))\n",
    "    LP1_list.append(LP1)\n",
    "    \n",
    "LP2_list = [GP2_list[-1]]\n",
    "for i in range (6, 0, -1):\n",
    "    LP2=  cv2.subtract(GP2_list[i-1], cv2.pyrUp(GP2_list[i]))\n",
    "    LP2_list.append(LP2)\n",
    "\n",
    "LS = [] \n",
    "for L1, L2 in zip(LP1_list, LP2_list):\n",
    "    h, w, c = L1.shape\n",
    "    L = np.hstack((L1[:, :w//2], L2[:, w//2:]))\n",
    "    LS.append(L)\n",
    "\n",
    "output = LS[0]\n",
    "for i in range(1, 7):\n",
    "    output = cv2.add(cv2.pyrUp(output), LS[i])\n",
    "    \n",
    "\n",
    "cv2.imshow(\"stiching image pyramid\", output)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "- Stitching horizontal menggunakan Image Pyramid\n",
    "\n",
    "<img src=\"resource/vstack.png\" style=\"width:300px\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Jawaban -----\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread(\"apple.jpg\")\n",
    "img2 = cv2.imread(\"orange.jpg\")\n",
    "\n",
    "GP1 = img1.copy()\n",
    "GP1_list = [GP1] \n",
    "for i in range (6):\n",
    "    GP1 = cv2.pyrDown(GP1)\n",
    "    GP1_list.append(GP1)\n",
    "\n",
    "GP2 = img2.copy()\n",
    "GP2_list = [GP2] \n",
    "for i in range (6):\n",
    "    GP2 = cv2.pyrDown(GP2)\n",
    "    GP2_list.append(GP2)\n",
    "    \n",
    "LP1_list = [GP1_list[-1]]\n",
    "for i in range (6, 0, -1):\n",
    "    LP1 =  cv2.subtract(GP1_list[i-1], cv2.pyrUp(GP1_list[i]))\n",
    "    LP1_list.append(LP1)\n",
    "    \n",
    "LP2_list = [GP2_list[-1]]\n",
    "for i in range (6, 0, -1):\n",
    "    LP2=  cv2.subtract(GP2_list[i-1], cv2.pyrUp(GP2_list[i]))\n",
    "    LP2_list.append(LP2)\n",
    "\n",
    "LS = [] \n",
    "for L1, L2 in zip(LP1_list, LP2_list):\n",
    "    h, w, c = L1.shape\n",
    "    L = np.vstack((L1[:w//2, :], L2[w//2:, :]))\n",
    "    LS.append(L)\n",
    "\n",
    "output = LS[0]\n",
    "for i in range(1, 7):\n",
    "    output = cv2.add(cv2.pyrUp(output), LS[i])\n",
    "    \n",
    "\n",
    "cv2.imshow(\"stiching image pyramid\", output)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Geometric Transformation\n",
    "<img src=\"resource/geometric_transformation.jpg\" style=\"width:600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Warp Affine\n",
    "- method `cv2/warpAffine(img, M, (w,h))`\n",
    "- where :\n",
    "    - `img` : input image \n",
    "    - `M` : input matrix (rotation/translation/scale)\n",
    "    - `(w,h)` : size output image \n",
    "\n",
    "### Rotate Image using Affine Transform\n",
    "\n",
    "- Matrix rotation : <br>\n",
    "$M = \\begin{bmatrix} cos(\\theta) & -sin(\\theta) \\\\ sin(\\theta) & cos(\\theta) \\end{bmatrix}$ <br>\n",
    "<br>\n",
    "where , <br>\n",
    "$M = \\begin{bmatrix} \\alpha & \\beta & (1-\\alpha)\\cdot c_x-\\beta\\cdot c_y \\\\ -\\beta   & \\alpha &   \\beta\\cdot c_x + (1-\\alpha)\\cdot c_y \\end{bmatrix}$ <br>\n",
    "<br>\n",
    "$\\alpha = scale \\cdot cos(\\theta)$<br>\n",
    "$\\beta = scale \\cdot sin(\\theta)$ <br><br>\n",
    "\n",
    "- method `cv2.getRotationMatrix2D(center, degre, scale)`\n",
    "- where :\n",
    "    - `center` : center of rotation (tuple), c/ (30,30)\n",
    "    - `degre` : rotation angel\n",
    "    - `scale` : image scale factor\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"lena.jpg\")\n",
    "h, w, c = img.shape\n",
    "\n",
    "center = (w // 2, h // 2)\n",
    "M = cv2.getRotationMatrix2D(center, 60, 1.0)\n",
    "rotated = cv2.warpAffine(img, M, (w, h))\n",
    "                                  \n",
    "cv2.imshow(\"rotated image\", rotated)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "- add trackbar, to change angel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = 360\n",
    "default_value = 10\n",
    "\n",
    "title_window = \"Rotate Image\"\n",
    "\n",
    "def on_trackbar(val):\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, val, 1.0)\n",
    "    rotated = cv2.warpAffine(img, M, (w, h))\n",
    "    cv2.imshow(title_window, rotated)\n",
    "\n",
    "img = cv2.imread('lena.jpg')\n",
    "w, h, c = img.shape\n",
    "\n",
    "cv2.namedWindow(title_window)\n",
    "cv2.createTrackbar('angel', title_window , default_value, max_value, on_trackbar)\n",
    "\n",
    "on_trackbar(default_value)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Translation\n",
    "- Translation matrix : <br>\n",
    "$M =\\begin{bmatrix} 1 & 0 & t_{x} \\\\ 0 & 1 & t_{y} \\end{bmatrix}_{2 \\times 3}$\n",
    "- **Negative** values of **tx** will shift the image to the **left**\n",
    "- **Positive** values of **tx** will shift the image to the **right**\n",
    "- **Negative** values of **ty** will shift the image **up**\n",
    "- **Positive** values of **ty** will shift the image **down**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('lena.jpg')\n",
    "h, w, c= img.shape\n",
    "M = np.float32([[1, 0, 100], \n",
    "                [0, 1, 50]])\n",
    "translated = cv2.warpAffine(img, M, (w, h))\n",
    "\n",
    "cv2.imshow(\"Image Translation\", translated)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine Tranform & Perspective Transform\n",
    "<img src=\"resource/TransformationsDifference.png\" style=\"width:500px\"></img>\n",
    "\n",
    "\n",
    "### Affine Transform\n",
    "- $M$ is 2x3 tranfromation matrix to pass throuh `cv2.warpAffine()`\n",
    "- method `cv.getAffineTransform(pts1, pts2)`\n",
    "- where :\n",
    "    - `pts1` :  triangle vertices source image\n",
    "    - `pts2` :  triangle vertices destination image <br>\n",
    "<img src=\"resource/Warp_Affine_Tutorial_Theory_0.jpg\" style=\"width:300px\"></img>\n",
    "- source image :\n",
    "    - point 0,0\n",
    "    - point w, 0\n",
    "    - point 0, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('lena.jpg')\n",
    "h, w, c= img.shape\n",
    "M = np.float32([[1, 0, 100], \n",
    "                [0, 1, 50]])\n",
    "\n",
    "pts1 = np.float32([[0,0],[w,0],[0,h]]) #source\n",
    "pts2 = np.float32([[0,100],[300,50],[50,350]]) # destination\n",
    "\n",
    "M = cv2.getAffineTransform(pts1, pts2)\n",
    "warp_dst = cv2.warpAffine(img, M, (w, h))\n",
    "\n",
    "cv2.imshow(\"Warp Transform\", warp_dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Translation on click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts1 = []\n",
    "pts2 = []\n",
    "windowName = \"Affine Transform by Mouse Click\"\n",
    "is_edit = False\n",
    " \n",
    "def affine_transform(event,x,y,flags,param):\n",
    "    \n",
    "    global pts1, pts2, is_edit, frame\n",
    "    \n",
    "    if event == cv2.EVENT_RBUTTONDOWN:\n",
    "        is_edit = True\n",
    "        pts1 = []\n",
    "        pts2 = []\n",
    "        frame = img.copy()\n",
    "        \n",
    "    if event == cv2.EVENT_LBUTTONDOWN and is_edit:\n",
    "        if len(pts1) < 3 :\n",
    "            pts1.append([x, y])\n",
    "            cv2.circle(frame, (x,y), 6, (255,255,0), -1)\n",
    "            \n",
    "        elif len(pts2) < 3 :\n",
    "            if len(pts2) == 0:\n",
    "                frame = np.zeros((frame.shape)).astype(np.uint8)\n",
    "            pts2.append([x, y])\n",
    "            cv2.circle(frame, (x,y), 6, (0,255,255), -1)\n",
    "        else :\n",
    "            w, h, c = frame.shape\n",
    "            pts1 = np.float32(pts1)\n",
    "            pts2 = np.float32(pts2)\n",
    "            M = cv2.getAffineTransform(pts1, pts2)\n",
    "            frame = cv2.warpAffine(img.copy(), M, (w, h))\n",
    "            is_edit = False\n",
    "                                      \n",
    "cv2.namedWindow(windowName) \n",
    "cv2.setMouseCallback(windowName, affine_transform) \n",
    "\n",
    "img = cv2.imread(\"lena.jpg\")\n",
    "frame = img.copy()\n",
    "while True:\n",
    "    cv2.imshow(windowName, frame)\n",
    "    if cv2.waitKey(1) == ord('q') :\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perspective Transformation\n",
    "\n",
    "- M is 3x3 transformation matrix for Perspective Transform.\n",
    "- to create M matrix, we need **4 points on the input image** and corresponding points on the **output image**\n",
    "- method `cv2.getPerspectiveTransform(pts1, pts2)`\n",
    "- where :\n",
    "    - `pts1` : four vertices source image\n",
    "    - `pts2` : four vertices destination image <br>\n",
    "<img src=\"resource/perspective.png\" style=\"width:400px\"></img> \n",
    "- sudoku image problem : <br>\n",
    "<img src=\"resource/sudoku.png\" style=\"width:400px\"></img> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('sudoku.jpg')\n",
    "h, w, c = img.shape\n",
    "\n",
    "# tl, tr, br, bl\n",
    "pts1 = np.float32([[56,65],[368,52],[389,390], [28,387]])\n",
    "pts2 = np.float32([[0,0],[300,0],[300,300],[0,300]])\n",
    "\n",
    "M = cv2.getPerspectiveTransform(pts1,pts2)\n",
    "\n",
    "output = cv2.warpPerspective(img, M, (300,300))\n",
    "\n",
    "for x, y in pts1.astype(np.uint16):\n",
    "    cv2.circle(img, (x,y), 4, (255, 255, 0), -1)\n",
    "\n",
    "cv2.imshow(\"Original Image\", img)\n",
    "cv2.imshow(\"Perspective Transform Image\", output)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using trackbar to put the coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_points(pts):\n",
    "    rect = np.zeros((4, 2), dtype = \"float32\")\n",
    "    s = pts.sum(axis = 1)\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "    diff = np.diff(pts, axis = 1)\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "    # tl, tr, br, bl\n",
    "    return rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tl, tr, br, bl\n",
    "pts1 = []\n",
    "pts2 = np.float32([[0,0],[300,0],[300,300], [0,300]])\n",
    "windowName = \"Perspective Transform by Mouse Click\"\n",
    "is_edit = False\n",
    " \n",
    "def perspective_transform(event,x,y,flags,param):\n",
    "    \n",
    "    global pts1, is_edit, frame\n",
    "    \n",
    "    if event == cv2.EVENT_RBUTTONDOWN:\n",
    "        is_edit = True\n",
    "        pts1 = []\n",
    "        frame = img.copy()\n",
    "        \n",
    "    if event == cv2.EVENT_LBUTTONDOWN and is_edit:\n",
    "        if len(pts1) < 4 :\n",
    "            pts1.append([x, y])\n",
    "            cv2.circle(frame, (x,y), 4, (255,255,0), -1)\n",
    "            \n",
    "        else :\n",
    "            #pts1 = order_points(np.array(pts1))\n",
    "            pts1 = np.float32(pts1)\n",
    "            M = cv2.getPerspectiveTransform(pts1,pts2)\n",
    "            output = cv2.warpPerspective(img, M, (300,300))\n",
    "            cv2.imshow(\"Perpective Transform Result\", output)\n",
    "            is_edit = False\n",
    "                                      \n",
    "cv2.namedWindow(windowName) \n",
    "cv2.setMouseCallback(windowName, perspective_transform) \n",
    "\n",
    "img = cv2.imread(\"sudoku.jpg\")\n",
    "frame = img.copy()\n",
    "while True:\n",
    "    cv2.imshow(windowName, frame)\n",
    "    if cv2.waitKey(1) == ord('q') :\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penerapan Affine Transform untuk crop foto ktp dengan perspective transform\n",
    "<img src=\"resource/crop_ktp.png\" style=\"width:400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- read image\n",
    "- convert ke grayscale\n",
    "- apply thresholding OTSU\n",
    "- find contour\n",
    "- filter contour \n",
    "    - drop small contour, --> cv2.boundingRect()\n",
    "    - drop small area, --> cv2.contourArea()\n",
    "    - drop to large area --> cv2.contourArea()\n",
    "- find coordinate of photo (x1,y1), (x2,y2), (x3,y3), (x4,y4) \n",
    "    - find perimeter (keliling) --> cv2.approxPolyDP()\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contours(img):\n",
    "    # First make the image 1-bit and get contours\n",
    "    imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    ret, thresh = cv2.threshold(imgray, 0, 255, cv2.THRESH_OTSU)\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # filter contours that are too large or small\n",
    "    h, w, c = img.shape\n",
    "    size = h*w\n",
    "    contours = [cc for cc in contours if contourOK(cc, size)]\n",
    "    return contours\n",
    "\n",
    "\n",
    "def contourOK(cc, size=1000000):\n",
    "    x, y, w, h = cv2.boundingRect(cc)\n",
    "    if w < 50 or h < 50: \n",
    "        return False # too narrow or wide is bad\n",
    "    area = cv2.contourArea(cc)\n",
    "    return area < (size * 0.25) and area > 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_4_coord(contours):\n",
    "    perimeter = []\n",
    "    for cc in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cc)\n",
    "\n",
    "        perimeter = cv2.approxPolyDP(cc, 0.09 * cv2.arcLength(cc, True), True)\n",
    "    return np.array(perimeter[:,0,:])\n",
    "        \n",
    "def order_points(pts):\n",
    "\n",
    "    rect = np.zeros((4, 2), dtype = \"float32\")\n",
    "    s = pts.sum(axis = 1)\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "    diff = np.diff(pts, axis = 1)\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "    return rect\n",
    "\n",
    "def transform(img, pts):\n",
    "\n",
    "    rect = order_points(pts)\n",
    "    (tl, tr, br, bl) = rect\n",
    "    print(tl, tr, br, bl)\n",
    "\n",
    "    widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
    "    widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
    "    maxWidth = max(int(widthA), int(widthB))\n",
    "\n",
    "    heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
    "    heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
    "    maxHeight = max(int(heightA), int(heightB))\n",
    "\n",
    "    dst = np.array([\n",
    "        [0, 0],\n",
    "        [maxWidth - 1, 0],\n",
    "        [maxWidth - 1, maxHeight - 1],\n",
    "        [0, maxHeight - 1]], dtype = \"float32\")\n",
    "\n",
    "    M = cv2.getPerspectiveTransform(rect, dst)\n",
    "    warped = cv2.warpPerspective(img, M, (maxWidth, maxHeight))\n",
    "    return warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('ktp4.jpg')\n",
    "\n",
    "contours = get_contours(img)\n",
    "pts = find_4_coord(contours)\n",
    "\n",
    "img_transform = transform(img, pts)\n",
    "\n",
    "\n",
    "cv2.imshow(\"Croped Photo\", cv2.resize(img_transform, (0,0), fx=2, fy=2))\n",
    "cv2.imshow(\"Original\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sumber\n",
    "- [tutorial_warp_affine](https://docs.opencv.org/master/d4/d61/tutorial_warp_affine.html)\n",
    "- [py_geometric_transformations](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:opencv_env]",
   "language": "python",
   "name": "conda-env-opencv_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
